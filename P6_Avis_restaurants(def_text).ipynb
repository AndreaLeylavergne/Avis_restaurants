{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wPomInoR9ROI"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1701588289419,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "4AkMgSjV_evK",
    "outputId": "d38a3175-6a69-4736-c25f-484679c45826",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NHv767CkSQoP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:48:45,492 : INFO : Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-12-12 08:48:45,493 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hYQ3yZkpkcPM"
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6586,
     "status": "ok",
     "timestamp": 1701588518704,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "v1TDrUI6khFz",
    "outputId": "08e1b13a-5110-4eb2-9390-bef77b925ee5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: voila in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: jupyter-client<9,>=7.4.4 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core>=4.11.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (5.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.0.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.3.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (2.25.1)\n",
      "Requirement already satisfied: nbclient<0.8,>=0.4.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (0.5.13)\n",
      "Requirement already satisfied: nbconvert<8,>=6.4.5 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (7.10.0)\n",
      "Requirement already satisfied: traitlets<6,>=5.0.3 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (5.7.1)\n",
      "Requirement already satisfied: websockets>=9.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from voila) (10.4)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-client<9,>=7.4.4->voila) (0.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-client<9,>=7.4.4->voila) (1.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-client<9,>=7.4.4->voila) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-client<9,>=7.4.4->voila) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-client<9,>=7.4.4->voila) (6.3.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-core>=4.11.0->voila) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-core>=4.11.0->voila) (305.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (3.5.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (3.1.2)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (5.9.2)\n",
      "Requirement already satisfied: overrides in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (7.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (23.1)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (0.14.1)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-server<3,>=2.0.0->voila) (0.58.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyterlab-server<3,>=2.3.0->voila) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyterlab-server<3,>=2.3.0->voila) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyterlab-server<3,>=2.3.0->voila) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyterlab-server<3,>=2.3.0->voila) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (2.1.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (2.0.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbconvert<8,>=6.4.5->voila) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.0->voila) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.0->voila) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.3.0->voila) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from bleach!=5.0.0->nbconvert<8,>=6.4.5->voila) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from bleach!=5.0.0->nbconvert<8,>=6.4.5->voila) (0.5.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (0.10.6)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.0.0->voila) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.0.0->voila) (6.0)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.0.0->voila) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.0.0->voila) (0.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.0.0->voila) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.3.0->voila) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.3.0->voila) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.3.0->voila) (2023.11.17)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=2.0.0->voila) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from beautifulsoup4->nbconvert<8,>=6.4.5->voila) (2.5)\n",
      "Requirement already satisfied: fqdn in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (2.4)\n",
      "Requirement already satisfied: uri-template in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.0.0->voila) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.0.0->voila) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.3.0->voila) (2.8.19.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbclassic nbconvert notebook qtconsole run script\n",
      "server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbclassic nbconvert notebook qtconsole run script\n",
      "server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbclassic nbconvert notebook qtconsole run script\n",
      "server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-serverextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install voila\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "!jupyter nbextension enable --py voila --sys-prefix\n",
    "!jupyter serverextension enable --py voila --sys-prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlQldlxTIm3S"
   },
   "source": [
    "# *Analyse de texte et d'images*\n",
    "\n",
    "Problématique : L'entreprise Avis restau souhaite améliorer sa plateforme avec une nouvelle fonctionnalité de collaboration. Les utilisateurs pourront par exemple poster des avis et des photos sur leur restaurant préféré. Cette initiative devra êrùettre à l’entreprise de mieux comprendre les avis postés par les utilisateurs.\n",
    "\n",
    "Méthodologie : Il est demandé de proposer un outil qui permette de faire une analyse de faisabilité pour la mise en place de cet outil à partir de trois grands groupes d'éléments: \n",
    "1. les avis des utilisateurs afin de comprendre, le cas échéant, des éventuelles raisons d'insatisfaction,\n",
    "2. des photos postées par les utilisateurs directement sur le site et,\n",
    "3. un travail de récupération de nouvelles données sur les restaurant afin de vérifier que l'outil est assez robuste pour l'envoyer en production.\n",
    "\n",
    "Méthodologie technique :\n",
    "\n",
    "1. Analyse des avis des clients\n",
    "\n",
    "> - Préparation et filtrage des données ne gardant que les commentaires négatifs\n",
    "> - Analyse (bigrammes, trigrammes et pondération)\n",
    "> - Identification des sujets les plus saillants (LDA, T-sne et explicabilité)\n",
    "> - Apercu de l'analyse de sentiments (TextBlob)\n",
    "\n",
    "2. Analyse des images postés par les clients\n",
    "> - Préparation des données (Création des features)\n",
    "> - Analyse par une méthode non-supérvisée (ACP, clustering, T-SNE, Kmeans)\n",
    "> - SIFT\n",
    "> - Modèle VGG16 (Transfer learning)\n",
    "> - Approche ImageDatagenerator avec data augmentation\n",
    "> - Approche sans data augmentation\n",
    "> - Approche avec data augmentation intégrée au modèle\n",
    "\n",
    "3. Récupération des données et création d'un fichier csv pour faciliter leur utilisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XROYyJWLZJqH"
   },
   "source": [
    "# 1. Analyse des avis des clients (travail sur le texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cu6LBcuo-7sI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_text.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1701413119368,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "5-OTZ9ZdAzaI",
    "outputId": "59962f2c-9748-49cd-e458-4fefe417c490"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WUGRmrfSzQCyeG6mYAM-vw</td>\n",
       "      <td>The food is just average and very overpriced a...</td>\n",
       "      <td>Restaurants, Latin American, Mexican, Tacos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3wogQmKw729tKKWaVbZvbg</td>\n",
       "      <td>I paid $20 for the gator plate which was the s...</td>\n",
       "      <td>Herbs &amp; Spices, Seafood, Restaurants, Cajun/Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEh5BPKE8LHm4PUsH_E1ig</td>\n",
       "      <td>I really like the staff behind the counter and...</td>\n",
       "      <td>Italian, Salad, Pizza, Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oj-XIt4Xj6NSsHzlSt-PsA</td>\n",
       "      <td>This is not Chinese food.\\n\\nIt's like aliens ...</td>\n",
       "      <td>Restaurants, Gluten-Free, Asian Fusion, Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfkE9AOK5zmiJ9nX0jo5tg</td>\n",
       "      <td>Definitely not like it used to be. I ordered t...</td>\n",
       "      <td>Restaurants, Burgers, American (Traditional)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  WUGRmrfSzQCyeG6mYAM-vw  The food is just average and very overpriced a...   \n",
       "1  3wogQmKw729tKKWaVbZvbg  I paid $20 for the gator plate which was the s...   \n",
       "2  DEh5BPKE8LHm4PUsH_E1ig  I really like the staff behind the counter and...   \n",
       "3  Oj-XIt4Xj6NSsHzlSt-PsA  This is not Chinese food.\\n\\nIt's like aliens ...   \n",
       "4  tfkE9AOK5zmiJ9nX0jo5tg  Definitely not like it used to be. I ordered t...   \n",
       "\n",
       "                                          categories  \n",
       "0        Restaurants, Latin American, Mexican, Tacos  \n",
       "1  Herbs & Spices, Seafood, Restaurants, Cajun/Cr...  \n",
       "2                 Italian, Salad, Pizza, Restaurants  \n",
       "3  Restaurants, Gluten-Free, Asian Fusion, Chines...  \n",
       "4       Restaurants, Burgers, American (Traditional)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1701413132455,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "Su15hwwxA17a",
    "outputId": "ea50b452-3248-4325-e4ef-0214e2e134d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w_YXxc_7BhbY"
   },
   "outputs": [],
   "source": [
    "df = df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1701409603756,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "D1WCycCMDlyO",
    "outputId": "dda95157-1c0f-43d7-cdce-05efe6914d2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1701413141242,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "D2mpPsANDoLZ",
    "outputId": "fc06a1c3-db76-450d-9ff5-1029588e5ffe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is just average and very overpriced a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I paid $20 for the gator plate which was the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like the staff behind the counter and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is not Chinese food.\\n\\nIt's like aliens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Definitely not like it used to be. I ordered t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The food is just average and very overpriced a...\n",
       "1  I paid $20 for the gator plate which was the s...\n",
       "2  I really like the staff behind the counter and...\n",
       "3  This is not Chinese food.\\n\\nIt's like aliens ...\n",
       "4  Definitely not like it used to be. I ordered t..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNSgvzckgUUv"
   },
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edX7WRq2HXiI"
   },
   "source": [
    "#### Extraire une fraction du df pour travailler plus facilement avec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hwu7e7OfF3jb"
   },
   "outputs": [],
   "source": [
    "# Extraction d'une fraction pour travailler + facilement\n",
    "df_1 = df\n",
    "#df_1 = df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1701413171825,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "iB3mwj_6GPw5",
    "outputId": "d0c6a276-50e7-4399-b773-24abe938ad6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY6QucfJHb0v"
   },
   "source": [
    "## 1. Travail sur les documents disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4CzkiVpAP3K"
   },
   "source": [
    "### Enlèvement de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1701413173191,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "eqatYsOFAATj",
    "outputId": "fecd063e-fce1-4284-e0be-e0303f68b620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDZhetQTFP-l"
   },
   "source": [
    "#### 1. Initialisation des dependances nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701413173191,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "sX0N5Q2EOgLj",
    "outputId": "acfedf94-97e7-40ff-ca2e-60b9e051a693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UOBYf0iXFAmu"
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX5_GjcgFph0"
   },
   "source": [
    "##### Définition d'une fonction de preprocessing : tokenization, lemmatisation, minuscules, filtrage alphabétique, enlèvement des adverbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5505,
     "status": "ok",
     "timestamp": 1701413178695,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "wNSh0HXtQlDB",
    "outputId": "acd63005-0943-4cfb-e800-4247d6f2769d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\envs\\p6_texte_image\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ8RrTjjQHrC"
   },
   "source": [
    "### 2. Suppression de ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nke2Vfvibs6"
   },
   "source": [
    "2.1.  Fonction de cleaning numéro 1 : fonction pour enlèver la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vpiRrVO1Q2rS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def supprimer_ponctuation(df_1):\n",
    "    # Initialize an empty list to store preprocessed text\n",
    "    preprocess_list = []\n",
    "\n",
    "    # Iterate through each text entry in the 'Text' column\n",
    "    for text in df_1['text']:\n",
    "        # Create a tokenizer object\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "        # Tokenize the text, removing punctuation\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # Join the tokens back into a preprocessed text string\n",
    "        preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "        # Append the preprocessed text to the list\n",
    "        preprocess_list.append(preprocessed_text)\n",
    "\n",
    "    # Assign the preprocess_list to the 'Text' column of the DataFrame\n",
    "    df_1['text'] = preprocess_list\n",
    "\n",
    "    return df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701413178696,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "ywzuTLZRkv6E",
    "outputId": "c4924311-60d6-47bb-c0cb-79df7943593c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is just average and very overpriced a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I paid $20 for the gator plate which was the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like the staff behind the counter and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is not Chinese food.\\n\\nIt's like aliens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Definitely not like it used to be. I ordered t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The food is just average and very overpriced a...\n",
       "1  I paid $20 for the gator plate which was the s...\n",
       "2  I really like the staff behind the counter and...\n",
       "3  This is not Chinese food.\\n\\nIt's like aliens ...\n",
       "4  Definitely not like it used to be. I ordered t..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIC64hfFih2k"
   },
   "source": [
    "Appel de la fonction qui enlève la pontuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1701413179714,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "j3DwAwydh1HU",
    "outputId": "b63cb494-4cc0-4b94-9a96-999cb1861cd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is just average and very overpriced a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I paid 20 for the gator plate which was the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like the staff behind the counter and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is not Chinese food It s like aliens land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Definitely not like it used to be I ordered th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The food is just average and very overpriced a...\n",
       "1  I paid 20 for the gator plate which was the sa...\n",
       "2  I really like the staff behind the counter and...\n",
       "3  This is not Chinese food It s like aliens land...\n",
       "4  Definitely not like it used to be I ordered th..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the supprimer_ponctuation function to preprocess the text\n",
    "df_1 = supprimer_ponctuation(df_1)\n",
    "\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IYZilLlZlclT"
   },
   "outputs": [],
   "source": [
    "# Vérification de l'opération\n",
    "#index_to_find = 27804\n",
    "\n",
    "# loc pour trouver l'index\n",
    "#row = df_1.loc[index_to_find]\n",
    "\n",
    "# afficahge de la ligne avec row\n",
    "#print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJTRz8ZOHV0g"
   },
   "source": [
    "### 3. Fonction de cleaning numéro 2 : fonction de pré-processing\n",
    "\n",
    "- Tokenisation\n",
    "- Lower-case\n",
    "- Filtrage par langue pour ne garder que les mots en anglais\n",
    "- Enlèvement des adverbes\n",
    "- Enlèvement de stopwords\n",
    "- Lémmatisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701413179715,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "IizP6EfLQsNm",
    "outputId": "e936cd1e-0e22-4659-ead3-e28906f7f3c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Preprocess_listofSentence(listofSentence):\n",
    "    preprocess_list = []\n",
    "    for sentence in listofSentence:\n",
    "        sentence_lower = sentence.lower()  # Convertir les mots en minuscules\n",
    "        english_words = set(nltk.corpus.words.words())  # Ne garder que les mots qui sont en anglais\n",
    "        sentence_clean = \" \".join(w for w in nltk.wordpunct_tokenize(sentence_lower) if w in english_words or not w.isalpha())\n",
    "\n",
    "        tokenize_sentence = word_tokenize(sentence_clean)\n",
    "\n",
    "        # Part-of-speech tagging to identify adverbs\n",
    "        pos_tags = nltk.pos_tag(tokenize_sentence)\n",
    "        words_w_adverbs = [word for word, pos in pos_tags if pos not in ('RB', 'RBR', 'RBS', 'JJR', 'JJS')]\n",
    "\n",
    "        words_w_stopwords = [word for word in words_w_adverbs if word not in stopwords]\n",
    "\n",
    "        # Filtrer les mots qui ont moins de 3 lettres\n",
    "        words_filtered = [word for word in words_w_stopwords if len(word) >= 3]\n",
    "\n",
    "        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_filtered)\n",
    "\n",
    "        sentence_clean = ' '.join(w for w in words_lemmatize)\n",
    "\n",
    "        preprocess_list.append(sentence_clean)\n",
    "\n",
    "    return preprocess_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi01myXYmY1k"
   },
   "source": [
    "### Appel de la fonction de cleaning n.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYxX09WNF4Kg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_list = Preprocess_listofSentence(df_1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1701415882062,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "TCcHMwdJH3sm",
    "outputId": "f30163fd-18a9-4813-b8f6-d368f6735485"
   },
   "outputs": [],
   "source": [
    "#Vérification de l'opération\n",
    "type(preprocess_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1701415882062,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "r7Y6g1Tm2Z9j",
    "outputId": "0aa11a17-5b1f-4738-f22f-d878651f7d5a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Créer un DataFrame avec deux colonnes : une pour le texte original et une pour le texte prétraité\n",
    "df_1 = pd.DataFrame({'Original_Text': df_1['text'], 'Processed_Text': preprocess_list})\n",
    "\n",
    "# Augmentez la largeur d'affichage pour voir les colonnes côte à côte\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Afficher les deux colonnes côte à côte\n",
    "print(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1701415882063,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "RLpfFEOs27CW",
    "outputId": "427d7de1-991d-4074-b4fb-47c68201136f"
   },
   "outputs": [],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1701415882064,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "eooF1p8CDce2",
    "outputId": "276baabf-dcc8-4341-a3c1-c06aeb8ee2eb"
   },
   "outputs": [],
   "source": [
    "#application de la fonctionalité len() sur text avec une lambda sur le text pour voir la longueur de chaque entrée\n",
    "df_1.Original_Text.apply(lambda i : len(i)) #on obtient un vecteur indiquant la longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1701415882064,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "i2LRnpUHD7B3",
    "outputId": "d7747a5c-dce6-417a-b9bf-433bc014d12d"
   },
   "outputs": [],
   "source": [
    "#ajout d'une colonne pour voir la logueur du commentaire\n",
    "df_1[\"_len_txt_orig\"] = df_1.Original_Text.apply(lambda i : len(i))\n",
    "df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1701415882064,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "QftkwtEhIIpc",
    "outputId": "19b2cd71-ede2-4094-b494-156ee556d881"
   },
   "outputs": [],
   "source": [
    "#application de la fonctionalité len() sur text avec une lambda sur le text pour voir la longueur de chaque entrée\n",
    "#df_1.Processed_Text.apply(lambda i : len(i)) #on obtien un vecteur indiquant la longueur\n",
    "#ajout d'un colonne pour voir la logueur du commentaire\n",
    "df_1[\"_len_txt_proc\"] = df_1.Processed_Text.apply(lambda i : len(i))\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1701415882065,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "WncuB9uNLtB-",
    "outputId": "f80143de-9c88-4cb8-8279-a84ed590d9e6"
   },
   "outputs": [],
   "source": [
    "# Classement pour analyse simple de la longueur des commentaires par ordre croissant\n",
    "df_1.sort_values(\"_len_txt_orig\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1701415882065,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "V6jWQyIZ5r7T",
    "outputId": "86120af0-98d4-4d60-c2ef-350f720bf8df"
   },
   "outputs": [],
   "source": [
    "# Classement pour analyse simple de la longueur des commentaires par ordre décroissant\n",
    "df_1.sort_values(\"_len_txt_orig\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1701415882065,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "QSpisr7x3ITu",
    "outputId": "f0dd0e7b-0a89-477f-a130-9883f3fda15a"
   },
   "outputs": [],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPcfGZUsHQ4y"
   },
   "source": [
    "##### Analyse des mots fréquents qui ne sont pas informatifs de l'insatisfaction des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1701415882066,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "5FCeZ2xc8_q6",
    "outputId": "1cb6aae9-1d4a-44cb-c456-8b457f77c9c7"
   },
   "outputs": [],
   "source": [
    "# Convertir les valeurs de la colonne \"Processed_Text\" en chaînes de caractères avant la construction d'un \"raw_corpus\"\n",
    "df_1['Processed_Text'] = df_1['Processed_Text'].astype(str)\n",
    "\n",
    "#join all corpus\n",
    "raw_corpus = \"\".join(df_1.Processed_Text.values)\n",
    "raw_corpus[:1_000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2810,
     "status": "ok",
     "timestamp": 1701415885287,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "9zYwaZlVHR8q",
    "outputId": "1763498d-7169-4879-bd3e-58d9100c8d95"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# extraction des tokens dans une variable pour continuer le nettoyage\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Calculer la fréquence des mots\n",
    "word_freq = FreqDist(corpus_tokens)\n",
    "\n",
    "# Afficher les mots les plus fréquents (par exemple, les 10 premiers)\n",
    "top_words = word_freq.most_common(10)\n",
    "print(top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3649,
     "status": "ok",
     "timestamp": 1701415889287,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "jT7RI5n8JByQ",
    "outputId": "4059edd9-7a56-4e79-8b01-c7541f6f1fd1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# extraction des tokens dans une variable pour continuer le nettoyage\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Calculer la fréquence des mots\n",
    "word_freq = FreqDist(corpus_tokens)\n",
    "\n",
    "# Afficher les mots les plus fréquents (par exemple, les 50 premiers)\n",
    "top_words = word_freq.most_common(100)\n",
    "print(top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZD89lbBfKy6"
   },
   "source": [
    "Comptage des tokens pour contrôle avant l'opération d'élimination des mots le splus courants peu significatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1701415889287,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "aBqXzbIofGei",
    "outputId": "36a01212-a5c6-45c0-a927-4702f8215281"
   },
   "outputs": [],
   "source": [
    "#longuer de la liste de tokens\n",
    "len(corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701415889287,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "iNqaHA0QfGej",
    "outputId": "95e4914e-00e1-4d7e-8bab-b8a0f7a26f89"
   },
   "outputs": [],
   "source": [
    "#longuer de la liste de tokens uniques\n",
    "len(set(corpus_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR0nHZccYK9G"
   },
   "source": [
    "##### Elimination des mots fréquents non-informatifs de l'insatisfaction des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6847,
     "status": "ok",
     "timestamp": 1701415896132,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "cByhmsFSeyCU",
    "outputId": "a6de2757-15ae-4ec7-b498-7a6dc391ba09"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Extraction des tokens dans une variable pour continuer le nettoyage\n",
    "df_1_processed = nltk.word_tokenize(\" \".join(df_1['Processed_Text']))\n",
    "\n",
    "# Calculer la fréquence des mots\n",
    "word_freq = FreqDist(df_1_processed)\n",
    "\n",
    "# Afficher les mots les plus fréquents (par exemple, les 100 premiers)\n",
    "top_words = [word for word, freq in word_freq.most_common(100)]\n",
    "\n",
    "# Liste de mots à conserver\n",
    "words_to_keep = ['bad', 'another', 'worst', 'rude', \"wasn't\", 'horrible', 'cold', 'last', 'customer', 'service', 'wasnt', 'worst', 'terrible']\n",
    "\n",
    "# Liste pour stocker les mots qui ne font pas partie de top_words mais qui sont dans words_to_keep\n",
    "filtered_tokens = [word for word in df_1_processed if word not in top_words or word in words_to_keep]\n",
    "\n",
    "# Rejoindre les mots filtrés pour former un nouveau texte\n",
    "filtered_corpus = \" \".join(filtered_tokens)\n",
    "\n",
    "# Les mots de la liste words_to_keep seront conservés, même s'ils sont courants\n",
    "corpus_tokens = nltk.word_tokenize(filtered_corpus)\n",
    "\n",
    "# Affichez les 10 premiers mots du corpus_tokens\n",
    "print(corpus_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3741,
     "status": "ok",
     "timestamp": 1701415899797,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "bIsVxo8N-Hm3",
    "outputId": "b9eecf6c-3016-4b78-b066-a25cab6632ed"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "# Calculer la fréquence des mots dans le corpus nettoyé\n",
    "word_freq = FreqDist(df_1_processed)\n",
    "\n",
    "# Calculer le nombre total de documents\n",
    "total_documents = len(df_1)\n",
    "\n",
    "# Définir un seuil pour la fréquence minimale (20%)\n",
    "min_frequency = 0.2 * total_documents\n",
    "\n",
    "# Sélectionner les mots qui apparaissent dans au moins 20% des textes\n",
    "common_words = [word for word, freq in word_freq.items() if freq >= min_frequency]\n",
    "\n",
    "# Liste de mots à conserver (en plus des mots communs)\n",
    "words_to_keep = ['bad', 'another', 'worst', 'rude', \"wasn't\", 'horrible', 'cold', 'last', 'customer', 'service', 'wasnt', 'worst', 'terrible']\n",
    "\n",
    "# Liste pour stocker les mots qui ne font pas partie de common_words mais qui sont dans words_to_keep\n",
    "filtered_tokens = [word for word in df_1_processed if word not in common_words or word in words_to_keep]\n",
    "\n",
    "# Rejoindre les mots filtrés pour former un nouveau texte\n",
    "filtered_corpus = \" \".join(filtered_tokens)\n",
    "\n",
    "# Les mots de la liste words_to_keep seront conservés, même s'ils sont courants\n",
    "corpus_tokens = nltk.word_tokenize(filtered_corpus)\n",
    "\n",
    "# Affichez les 10 premiers mots du corpus_tokens\n",
    "print(corpus_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1701415899799,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "8x3LPi1D3POJ",
    "outputId": "ce0110fe-a02d-4224-e1bc-74062e3ba8b6"
   },
   "outputs": [],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1701415899799,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "cU_FDSt6YWLD",
    "outputId": "27237e57-be50-4b9d-f1e9-dcf627586adc"
   },
   "outputs": [],
   "source": [
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2991,
     "status": "ok",
     "timestamp": 1701415902776,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "nHIPTDU6a-3o",
    "outputId": "78fd4f63-8490-4807-e717-72de013d105c"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Tokenize le corpus nettoyé\n",
    "corpus_tokens = nltk.word_tokenize(filtered_corpus)\n",
    "\n",
    "# Calculer la fréquence des mots dans le corpus nettoyé\n",
    "word_freq_after_cleaning = FreqDist(corpus_tokens)\n",
    "\n",
    "# Afficher les 10 mots les plus fréquents\n",
    "top_words_after_cleaning = word_freq_after_cleaning.most_common(20)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(top_words_after_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1701415902777,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "UY5ZyT107Ulj",
    "outputId": "96816677-4ffb-4a01-cfc4-00df3afd6336"
   },
   "outputs": [],
   "source": [
    "# Classement pour analyse simple de la longueur des commentaires par ordre croissant\n",
    "df_1.sort_values(\"_len_txt_orig\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1701415902778,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "wQ3SSRanfGei",
    "outputId": "f11c98cf-5b79-4034-cb85-5f0d19abe8bf"
   },
   "outputs": [],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df-9rYRRKtGJ"
   },
   "source": [
    "Comptage des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1701415902779,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "qUOsW8As9AbZ",
    "outputId": "31d7411d-8b8a-4568-e1ef-25f6ab6fcd22"
   },
   "outputs": [],
   "source": [
    "raw_corpus[:1_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1701415902780,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "DciulWaDKwBq",
    "outputId": "8d2a2e17-2058-410f-fb40-16d84cd25fc4"
   },
   "outputs": [],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1701415902780,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "BIUtq66-a9so",
    "outputId": "bb5ef620-8db8-425a-cf0d-5c71d536b32f"
   },
   "outputs": [],
   "source": [
    "#longuer de la liste de tokens\n",
    "len(corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1701415902780,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "caITGL6LdNNK",
    "outputId": "570c80ec-9c10-4c48-e85f-385c8f449881"
   },
   "outputs": [],
   "source": [
    "#longuer de la liste de tokens uniques\n",
    "len(set(corpus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1701415902781,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "9Yrvcqejg_FY",
    "outputId": "4f214479-27a0-42a7-af03-d5fb7d3390f1"
   },
   "outputs": [],
   "source": [
    "#value counts\n",
    "corpus_tokens_count = pd.Series(corpus_tokens).value_counts()\n",
    "corpus_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1701415902781,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "ObU1mZy0jKqo",
    "outputId": "4655775e-4612-4ad4-955a-11fa59562b7d"
   },
   "outputs": [],
   "source": [
    "corpus_tokens_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1701415903307,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "g8SIrMDUjd_-",
    "outputId": "ecf713ed-f6e6-43b8-87fa-c9c8d1233a15"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(corpus_tokens_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1701415903307,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "bhThYBprgPdU",
    "outputId": "3e761fe4-8aef-4ab0-9b32-f83d6988170c"
   },
   "outputs": [],
   "source": [
    "#30 most common tokens\n",
    "corpus_tokens_count.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SYEPzU0tR6K"
   },
   "source": [
    "##### Mots peu significatifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jRoAJzetY37"
   },
   "source": [
    "##### Mots uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1701415903308,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "mR9Pbxq2iI19",
    "outputId": "cf924a22-11f3-408c-db7a-d19d2835a4ba"
   },
   "outputs": [],
   "source": [
    "#30 last common tokens\n",
    "corpus_tokens_count.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1701415903308,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "cMIjg7-PkIcA",
    "outputId": "25ee6220-1c9f-46be-d8b3-e4c761034573"
   },
   "outputs": [],
   "source": [
    "# définition d'une liste d'unique words --> mots présents une seule fois et pas très utiles\n",
    "list_unique_words = corpus_tokens_count[corpus_tokens_count==1]\n",
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1701415903309,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "YSfxSgYCl-22",
    "outputId": "45e1ff5a-dccf-4e1e-9675-170e24f3434d"
   },
   "outputs": [],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1701415903310,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "MfO01aLvmqTY",
    "outputId": "a593a6dc-8e72-4090-cfba-f316b2c724a5"
   },
   "outputs": [],
   "source": [
    "# définition d'une liste d'unique words --> mots présents une seule fois et pas très utiles\n",
    "list_words_10 = corpus_tokens_count[corpus_tokens_count<=10]\n",
    "list_words_10[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1701415903311,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "LI8dctTdnLzx",
    "outputId": "0b4de46e-c86e-4c2b-8933-6cb0d42704d2"
   },
   "outputs": [],
   "source": [
    "len(list_words_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1701415903311,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "qfFUUOG-9xAu",
    "outputId": "ab0988a5-2326-483a-b2e2-17ef96ee9156"
   },
   "outputs": [],
   "source": [
    "len(corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1701415904266,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "mtdSH5UTMe2-",
    "outputId": "65b86a5a-f23f-4aa5-f9c1-95f1c8e03370"
   },
   "outputs": [],
   "source": [
    "#corpus_tokens = corpus_tokens\n",
    "\n",
    "#list_words_10 = corpus_tokens_count[corpus_tokens_count<=10]\n",
    "\n",
    "# Éliminer les mots de 'list_words_10' de 'corpus_tokens'\n",
    "corpus_tokens = [word for word in corpus_tokens if word not in list_words_10]\n",
    "\n",
    "len(corpus_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1701415904266,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "7X-fpVoKOs1v",
    "outputId": "ab0f57c7-f421-4e2d-c44b-57b8c416968f"
   },
   "outputs": [],
   "source": [
    "#longuer de la liste de tokens uniques\n",
    "len(set(corpus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfLGpTvct3or"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Fonction pour enlever les mots contenus dans list_words_10\n",
    "def remove_words(text, words_to_remove):\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [token for token in tokens if token not in words_to_remove]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Appliquer la fonction pour enlever les mots de la liste list_words_10 à la colonne 'Processed_Text'\n",
    "df_1['Processed_Text'] = df_1['Processed_Text'].apply(remove_words, args=(list_words_10,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1701415910004,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "J4wPdmZ13mUu",
    "outputId": "30e1189a-c904-4006-ba5f-90a75b4a06bf"
   },
   "outputs": [],
   "source": [
    "df_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1701415910004,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "58YX6eIYT3uR",
    "outputId": "971a477a-c2ba-4bbe-a282-863d065a521e"
   },
   "outputs": [],
   "source": [
    "#Vérification\n",
    "word_to_search = \"sickness\"\n",
    "\n",
    "# Utiliser str.contains pour filtrer les lignes contenant le mot recherché\n",
    "result = df_1[df_1['Processed_Text'].str.contains(word_to_search)]\n",
    "\n",
    "# Afficher le résultat\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vre_Jj1JVjOx"
   },
   "outputs": [],
   "source": [
    "#Drop de la phrase en alphabeth non-occidental  et insulte\n",
    "\n",
    "# Import de Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Identification des lignes contenant le mot d'insulte\n",
    "rows_to_drop = df_1['Original_Text'].str.contains('bitch', case=False)\n",
    "\n",
    "# Suppression des lignes correspondantes du DataFrame\n",
    "df_1 = df_1[~rows_to_drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1701415912336,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "0Bkno8MqB1XV",
    "outputId": "47d8abde-def8-438d-8465-6d445eb9cd44"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Calculer la taille de l'échantillon (20% de la taille originale)\n",
    "sample_size = int(0.2 * len(corpus_tokens))\n",
    "\n",
    "# Échantillonner 20% des tokens de la liste\n",
    "sampled_tokens = random.sample(corpus_tokens, sample_size)\n",
    "\n",
    "# Afficher le résultat\n",
    "#print(sampled_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DW0J27cBsYK"
   },
   "source": [
    "### Associer chaque mot du corpus à un identifiant clé unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2710,
     "status": "ok",
     "timestamp": 1701415915043,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "Z4HN670nBqds",
    "outputId": "1aa49b5b-1fa2-4edf-e3df-d9467f9698cd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Tokenize chaque document dans la colonne 'Processed_Text'\n",
    "corpus_tokens = df_1['Processed_Text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Créer un dictionnaire\n",
    "dictionary = corpora.Dictionary(corpus_tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y93jAU0sFSbq"
   },
   "source": [
    "Le dictionnaire a été construit ici avec un total de 3940 termes uniques extraits des 19937 documents du corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxHGYGdDSUdz"
   },
   "source": [
    "### Bigrammes, trigrammes et Ngrammes\n",
    "\n",
    "#### Utilisation du module Phrases de Gesim pour anlayser les bigrammes et les trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14412,
     "status": "ok",
     "timestamp": 1701415929388,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "QnlTYY5xW0Pl",
    "outputId": "d0903ce5-d6a1-40a7-c7f1-ed9a895cf0fc"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize chaque document dans la colonne 'Processed_Text'\n",
    "corpus_tokens = df_1['Processed_Text'].apply(nltk.word_tokenize).tolist()\n",
    "\n",
    "# Calculer le nombre total de bigrams possibles dans le corpus\n",
    "#total_bigrams = sum(len(tokens) - 1 for tokens in corpus_tokens)\n",
    "\n",
    "# Définir un pourcentage minimum d'apparition des bigrams\n",
    "#percentage_threshold = 0.80\n",
    "\n",
    "# Calculer le seuil en fonction du pourcentage\n",
    "#threshold = total_bigrams * percentage_threshold\n",
    "\n",
    "# Créer un modèle Phrases avec le seuil\n",
    "#bigram = Phrases(corpus_tokens, min_count=10, threshold=threshold)\n",
    "\n",
    "# Créer un Phraser pour accélérer la recherche des bigrams\n",
    "#bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# Appliquer le Phraser pour obtenir les textes avec les bigrams\n",
    "#text_with_bigrams = [bigram_phraser[tokens] for tokens in corpus_tokens]\n",
    "\n",
    "# Afficher les textes avec les bigrams\n",
    "#for text in text_with_bigrams:\n",
    "    #print(\" \".join(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1701415929947,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "uF8YWARUSR5W",
    "outputId": "b5462903-9fcf-4f73-8940-badd0f59ba29"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "# tokeniser le corpus pour en extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_bigrams = list(nltk.bigrams(corpus_tokens))\n",
    "\n",
    "# affichage\n",
    "print(corpus_bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3489,
     "status": "ok",
     "timestamp": 1701415933434,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "Q3Be8IxAINu4",
    "outputId": "282b4fd3-bedb-4eb0-80cf-238be2d0238e"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Tokeniser le corpus pour extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_bigrams = list(bigrams(corpus_tokens))\n",
    "\n",
    "# Calculer la fréquence des bigrammes\n",
    "freq_dist = FreqDist(corpus_bigrams)\n",
    "\n",
    "# Obtenir les 10 bigrammes les plus fréquents\n",
    "top_20_bigrams = freq_dist.most_common(20)\n",
    "\n",
    "# Afficher les 20 bigrammes les plus fréquents\n",
    "print(\"Les 20 bigrammes les plus fréquents sont :\")\n",
    "for bigram, frequency in top_20_bigrams:\n",
    "    print(bigram, \":\", frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4130,
     "status": "ok",
     "timestamp": 1701415937558,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "L8d5N4EIdUwj",
    "outputId": "53e35097-d9cb-493c-a190-3c0b126516d0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Tokeniser le corpus pour extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_bigrams = list(bigrams(corpus_tokens))\n",
    "\n",
    "# Calculer la fréquence des bigrammes\n",
    "freq_dist = FreqDist(corpus_bigrams)\n",
    "\n",
    "# Calculer le nombre total de bigrammes\n",
    "total_bigrams = len(corpus_bigrams)\n",
    "\n",
    "# Calculer le pourcentage d'occurrence pour chaque bigramme\n",
    "bigram_percentage = [(bigram, frequency / total_bigrams * 100) for bigram, frequency in freq_dist.items()]\n",
    "\n",
    "# Trier les bigrammes par pourcentage décroissant\n",
    "sorted_bigrams = sorted(bigram_percentage, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Obtenir les 10 bigrammes les plus fréquents\n",
    "top_10_bigrams = sorted_bigrams[:10]\n",
    "\n",
    "# Afficher les 10 bigrammes les plus fréquents avec leur pourcentage\n",
    "print(\"Les 10 bigrammes les plus fréquents avec leur pourcentage sont :\")\n",
    "for bigram, percentage in top_10_bigrams:\n",
    "    print(f\"{bigram} : {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5304,
     "status": "ok",
     "timestamp": 1701415942806,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "KS5yGhTUuK9k",
    "outputId": "77a1d553-faf0-44ef-a9fe-98c65326712e"
   },
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5912,
     "status": "ok",
     "timestamp": 1701415948709,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "-6G4PzvGuU61",
    "outputId": "47d3b2c0-df98-47bb-a543-dfdcd8b12c6c"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5814,
     "status": "ok",
     "timestamp": 1701415954507,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "vJKXBpQT3eRn",
    "outputId": "1dddd26a-1ae0-48e9-bbef-f89b61ea29a8"
   },
   "outputs": [],
   "source": [
    "! pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZhcKCvFGqzd"
   },
   "outputs": [],
   "source": [
    "#import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10412,
     "status": "ok",
     "timestamp": 1701415964907,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "X59UI8MMHI-_",
    "outputId": "5491912a-cc01-4d3e-c977-488e26a44306"
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df_1['Processed_Text'], min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[(df_1['Processed_Text'])], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1701415964908,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "iU570llKK_HZ",
    "outputId": "83ef389a-e4ab-418a-cb18-58cd1c50c520"
   },
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxxudednpoU-"
   },
   "source": [
    "##### Pondération des bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281669,
     "status": "ok",
     "timestamp": 1701416246507,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "IQVGZguzpnRp",
    "outputId": "1be6e342-a7ca-4932-ad32-d97119bcdce7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Tokeniser le corpus pour extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_bigrams = list(bigrams(corpus_tokens))\n",
    "\n",
    "# Calculer la fréquence des bigrammes\n",
    "freq_dist = FreqDist(corpus_bigrams)\n",
    "\n",
    "# Obtenir le nombre total de bigrammes\n",
    "total_bigrams = len(corpus_bigrams)\n",
    "\n",
    "# Créer un dictionnaire pour stocker les bigrammes pondérés\n",
    "weighted_bigrams = {}\n",
    "\n",
    "# Calculer la fréquence relative de chaque bigramme\n",
    "for bigram, frequency in freq_dist.items():\n",
    "    weighted_bigrams[bigram] = frequency / total_bigrams\n",
    "\n",
    "# Afficher les bigrammes pondérés\n",
    "#for bigram, weight in weighted_bigrams.items():\n",
    "    #print(bigram, \":\", weight)\n",
    "\n",
    "# Afficher les 10 premiers bigrammes pondérés\n",
    "count = 0\n",
    "for bigram, weight in weighted_bigrams.items():\n",
    "    print(bigram, \":\", weight)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1701416247601,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "6_a_PzLosRj2",
    "outputId": "0d90ea52-8950-4919-f8f9-7034e2db6327"
   },
   "outputs": [],
   "source": [
    "# Création d'un bag of bigrams\n",
    "corpus_bigrams = list(bigrams(corpus_tokens))\n",
    "\n",
    "# Créer un dictionnaire de bigrammes et leurs fréquences\n",
    "bigram_freq = FreqDist(corpus_bigrams)\n",
    "\n",
    "# Afficher les 10 premiers bigrammes\n",
    "for bigram, freq in bigram_freq.most_common(10):\n",
    "    print(bigram, \":\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3296,
     "status": "ok",
     "timestamp": 1701416250895,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "SM5AUEWnT9s7",
    "outputId": "ef4d42ed-5ca0-43ff-d511-8683cfc7d6bd"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "# tokeniser le corpus pour en extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_trigrams = list(nltk.trigrams(corpus_tokens))\n",
    "\n",
    "# affichage\n",
    "print(corpus_trigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3795,
     "status": "ok",
     "timestamp": 1701416254686,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "9Na2R3DBJFWb",
    "outputId": "cd92bf90-7b81-4b76-81a2-dd06cf9d1821"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Tokeniser le corpus pour extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_trigrams = list(trigrams(corpus_tokens))\n",
    "\n",
    "# Calculer la fréquence des bigrammes\n",
    "freq_dist_tr = FreqDist(corpus_trigrams)\n",
    "\n",
    "# Obtenir les 20 trigrammes les plus fréquents\n",
    "top_20_trigrams = freq_dist_tr.most_common(20)\n",
    "\n",
    "# Afficher les 20 trigrammes les plus fréquents\n",
    "print(\"Les 20 trigrammes les plus fréquents sont :\")\n",
    "for trigram, frequency in top_20_trigrams:\n",
    "    print(trigram, \":\", frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3074,
     "status": "ok",
     "timestamp": 1701416257757,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "DHq3nmtzUllX",
    "outputId": "e5cd1784-f844-4dff-dd96-e0fbb2e0d1ff"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "# tokeniser le corpus pour en extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_ngrams = list(nltk.ngrams(corpus_tokens, 5))\n",
    "\n",
    "# affichage\n",
    "print(corpus_ngrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3884,
     "status": "ok",
     "timestamp": 1701416261640,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "AEpO36cKKbSB",
    "outputId": "645b3827-5ea9-434a-d636-f4d6fd4e6885"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Tokeniser le corpus pour extraire les bigrammes\n",
    "corpus_tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Créer les bigrammes\n",
    "corpus_ngrams = list(nltk.ngrams(corpus_tokens, 5))\n",
    "\n",
    "# Calculer la fréquence des bigrammes\n",
    "freq_dist_tr = FreqDist(corpus_ngrams)\n",
    "\n",
    "# Obtenir les 10 trigrammes les plus fréquents\n",
    "top_10_ngrams = freq_dist_tr.most_common(10)\n",
    "\n",
    "# Afficher les 10 ngrammes les plus fréquents\n",
    "print(\"Les 10 ngrammes les plus fréquents sont :\")\n",
    "for ngram, frequency in top_10_ngrams:\n",
    "    print(ngram, \":\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvGlyeKIxqt4"
   },
   "source": [
    "### **Traîtement du corpus**\n",
    "\n",
    "#### Vectorisation avec One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1701416262533,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "x_TSajlwPBFy",
    "outputId": "6d9bceb6-6916-42b9-ab8d-0fbfcb429ffe"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Créer un vectoriseur de comptage (CountVectorizer)\n",
    "vectorizer = CountVectorizer(decode_error='ignore', stop_words='english', lowercase=True,)\n",
    "\n",
    "# Appliquer le vectoriseur sur le corpus prétraité\n",
    "corpus_vectorized = vectorizer.fit_transform(df_1['Processed_Text'])\n",
    "#corpus_vectorized = vectorizer.fit_transform(corpus_tokens)\n",
    "\n",
    "# Obtenir les noms des caractéristiques (mots)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Affichez l'ensemble du vocabulaire\n",
    "print(\"Ensemble du vocabulaire :\", feature_names)\n",
    "\n",
    "# Affichez la matrice vocabulaire-documents\n",
    "print(\"\\nMatrice vocabulaire-documents :\\n\", corpus_vectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1701416263115,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "mG6TA6W0hEyF",
    "outputId": "8a6d8ad1-9165-4bd4-f23c-8fb598d3e28b"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Créer un vectoriseur de comptage (CountVectorizer)\n",
    "vectorizer = CountVectorizer(decode_error='ignore', stop_words='english', lowercase=True,)\n",
    "\n",
    "# Appliquer le vectoriseur sur le corpus prétraité\n",
    "corpus_vectorized_1 = vectorizer.fit_transform(df_1['Processed_Text'])\n",
    "corpus_vectorized_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n14GB04Zho25"
   },
   "source": [
    "### Tranformation de la matrice creuse en matrice dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8cnJPm8iBL5"
   },
   "outputs": [],
   "source": [
    "# Transformation matrice creuse en matrice dense\n",
    "corpus_vectorized_dense = corpus_vectorized_1.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYghPkitlG19"
   },
   "source": [
    "#### Obtention des noms des caractéristiques (mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuqZ7xIrknOr"
   },
   "outputs": [],
   "source": [
    "# Obtenir les noms des caractéristiques (mots)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701416263469,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "VsfrmrHCkwhc",
    "outputId": "ca50c5a8-8db8-4fa9-a863-37d3e26f9018"
   },
   "outputs": [],
   "source": [
    "# voir les 20 premiers\n",
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701416263469,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "xEgAloC-k-0Z",
    "outputId": "7e83a803-0407-4b86-8724-92d7072aa333"
   },
   "outputs": [],
   "source": [
    "# voir les 20 derniers\n",
    "feature_names[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmO6gP2I1MB5"
   },
   "source": [
    "#### Bag of words : TF-IDF. Term Frequency-Inverse Document Frequency\n",
    "\n",
    "Ce calcul évalue l’importance d’un terme contenu dans un document, relativement à une collection. Si un mot rare est particulièrement présent dans un document, son TF-IDF sera très élevé.\n",
    "TF(t,i) = Nombre d’apparitions du terme t dans le document i / Nombre total de termes dans le document\n",
    "\n",
    "IDF(t) = log(Nombre de documents dans le corpus / Nombre de documents où t apparaît)\n",
    "\n",
    "D’où TF(t,i) * IDF(t) donne la pertinence de chaque document vis-à-vis d’un mot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1418,
     "status": "ok",
     "timestamp": 1701416264885,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "Kv6mIdA5yPwB",
    "outputId": "d3284955-39da-4530-b072-ef2ce85f3cec"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Créer une instance du vectoriseur TfidfVectorizer\n",
    "vectorizer_Tf = TfidfVectorizer(decode_error='ignore', stop_words='english')\n",
    "\n",
    "# Appliquer le vectoriseur sur la colonne 'Processed_Text'\n",
    "corpus_vectorised = vectorizer_Tf.fit_transform(df_1['Processed_Text'])\n",
    "\n",
    "# Convertir les caractéristiques (corpus_vectorised ici) en une représentation de matrice creuse\n",
    "corpus_vectorised = corpus_vectorised.toarray()\n",
    "\n",
    "# Afficher les dimensions de la matrice de caractéristiques\n",
    "print(\"Dimensions de la matrice de caractéristiques :\", corpus_vectorised.shape)\n",
    "\n",
    "# Afficher l'ensemble du vocabulaire\n",
    "print(\"Ensemble du vocabulaire :\", vectorizer_Tf.get_feature_names_out())\n",
    "\n",
    "# Afficher la matrice TF-IDF\n",
    "print(\"\\nMatrice TF-IDF \\n\", corpus_vectorised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ_3p9SGnR08"
   },
   "source": [
    "#### Processus detaillé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1701416265394,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "pbz1r_Z5nMpw",
    "outputId": "865fb9cb-22ba-4365-ce42-a7873564b46f"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Créer une instance du vectoriseur TfidfVectorizer\n",
    "vectorizer_Tf = TfidfVectorizer(decode_error='ignore', stop_words='english')\n",
    "\n",
    "# Appliquer le vectoriseur sur la colonne 'Processed_Text'\n",
    "corpus_vectorised = vectorizer_Tf.fit_transform(df_1['Processed_Text'])\n",
    "corpus_vectorised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuPz2RyKnffA"
   },
   "source": [
    "#### Transformation de la matrice éparse en matrice dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1701416266100,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "l4FpiTIFnnak",
    "outputId": "f5d68e68-530b-42be-91e1-1422830cf347"
   },
   "outputs": [],
   "source": [
    "corpus_vectorised_d = corpus_vectorised.todense()\n",
    "corpus_vectorised_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P54ZxHMPoVxB"
   },
   "outputs": [],
   "source": [
    "# Obtenir les noms des caractéristiques (mots)\n",
    "feature_names_Tf = vectorizer_Tf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701416266100,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "zHK9hbX7oVxC",
    "outputId": "9790fc01-a6db-42fc-e306-f01f104cdb9e"
   },
   "outputs": [],
   "source": [
    "# voir les 20 premiers\n",
    "feature_names_Tf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701416266100,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "1WQuKGTOoVxD",
    "outputId": "549baf81-e4f5-4f74-9095-a6c2a49b5c94"
   },
   "outputs": [],
   "source": [
    "# voir les 20 derniers\n",
    "feature_names_Tf[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29242,
     "status": "ok",
     "timestamp": 1701416295341,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "svOrxDEhLofn",
    "outputId": "386fff37-4f24-4fa2-f63d-7d35ff5fc251"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "dct = Dictionary(dataset)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "vector = model[corpus[0]]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 7003,
     "status": "ok",
     "timestamp": 1701416302342,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "_dDUxHdT-GIC",
    "outputId": "02520b62-0799-4a28-810c-77abd5085660"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concaténer tous les textes de la colonne 'Processed_text' en une seule chaîne\n",
    "all_processed_text = ' '.join(df_1['Processed_Text'])\n",
    "\n",
    "# Créer un objet WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_processed_text)\n",
    "\n",
    "# Afficher le nuage de mots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMXzxIf9TOf3"
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_spUxwv0Bfp7"
   },
   "source": [
    "#### Conversion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0GjFzPeB0rN"
   },
   "source": [
    "#### Appel d'un dictionnaire à partir des données préparées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-c2ULaAAlE2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3337,
     "status": "ok",
     "timestamp": 1701416305675,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "txWL1HT2Do9m",
    "outputId": "ccdca339-bc68-498f-a7b0-032e583ef98b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize chaque document dans la colonne 'Processed_Text'\n",
    "corpus_tokens = df_1['Processed_Text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Créer un dictionnaire\n",
    "dictionary = gensim.corpora.Dictionary(corpus_tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRwDViWIEK1A"
   },
   "source": [
    "##### Filtrage du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701416305675,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "hSUEuF8QEIyR",
    "outputId": "a911bfa7-79dc-47d1-a592-64e971b2b760",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJsJAFWCE2VM"
   },
   "source": [
    "Nous avons filtré le dictionnaire pour ne garder que les mots pouvant apparaitre dans un total d'entre 99 et 15 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzc68N32Fdgf"
   },
   "source": [
    "##### Transformation en format Bag OF Words (bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQQwwfIHE1wX"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le nombre de documents dans bow_corpus\n",
    "print(\"Nombre de documents dans bow_corpus :\", len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EceftXmF4CE"
   },
   "source": [
    "#### Exécution du LDA\n",
    "\n",
    "On applique le topic modeling à l’aide de la fonction LdaMulticore de gensim en prenant bien soin de préciser le nombre de topic à extraire du corpus, le mapping entre les identifiants des mots (entiers) et les mots (chaîne de caractères) et le nombre à effectuer d’itération dans le corpus.\n",
    "\n",
    "Le nombre de topic à extraire constitue une variable importante du modèle, son choix est souvent cruciale tout comme celui du nombres de classes en clustering par exemple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3020585,
     "status": "ok",
     "timestamp": 1701419327549,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "J1xQdQUOGQsm",
    "outputId": "f0c72511-5304-4174-e0b4-5d12b0be33a1"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "#Création du model LDA\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dictionary, passes = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1701419327549,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "9xccECHAFsMw",
    "outputId": "2f0a1fd5-cd1c-4c9d-a5fa-21e2c43262f6"
   },
   "outputs": [],
   "source": [
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701419327549,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "0V9wE-3cF2pY",
    "outputId": "592c601d-e51a-4f26-d955-39ce73a60c7e"
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "for idx, topic in lda_model.print_topics(-1) :\n",
    "    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN4ZZ9X0IwWX"
   },
   "source": [
    "#### Cohérence du modèle\n",
    "\n",
    "Les mesures de cohérence évaluent le degré de similitude sémantique entre les mots les mieux notés dans le topics. Ces mesures aident à faire la distinction entre les topics sémantiquement interprétables et les topics dû à des inférences statistiques.\n",
    "\n",
    "Un bon modèle LDA comporte une cohérence comprise entre 0.4 et 0.7 (au-delà et en dessous le modèle est très probablement erroné). La cohérence pour un modèle LDA est calculée en procédant aux étapes suivantes :\n",
    "\n",
    "Segmentation : création de paires de mots à partir de sous-ensembles ;\n",
    "Calcul des probabilités : calcul probabilité d’occurrence d’un mot ;\n",
    "Mesure de confirmation : vérification « dans quelle mesure » un sous-ensemble de mots supporte un autre sous-ensemble de mots dans chaque paire ;\n",
    "Agrégation : agrégation de toutes les valeurs calculées à l’étape précédente en une seule valeur qui est notre score final de cohérence de sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4869,
     "status": "ok",
     "timestamp": 1701419332414,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "lV5yM66VJPqB",
    "outputId": "93907ff6-91a2-4c2b-8e00-79049c9e5653"
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_tokens, dictionary=dictionary)\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WDzJHIezDXc"
   },
   "source": [
    "##### Stockage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsrRYbKJK_dW"
   },
   "outputs": [],
   "source": [
    "all_topic_model = []\n",
    "for i in range(len(topics)):\n",
    "  str = topics[i].split(' + ')\n",
    "  topic_model = []\n",
    "  for j in range(10):\n",
    "    weight = str[j][0:5]\n",
    "    word = str[j][7:len(str[j])-1]\n",
    "    topic_model.append((weight, word))\n",
    "  all_topic_model.append(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701419332414,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "JlQqBZbGLG_u",
    "outputId": "432db7cc-d908-4792-efcb-4da4cb5714b8"
   },
   "outputs": [],
   "source": [
    "df_topic_model = pd.DataFrame(all_topic_model)\n",
    "df_topic_model.rename(index = {0: \"Topic 1\", 1: \"Topic 2\", 2: \"Topic 3\", 3: \"Topic 4\", 4: \"Topic 5\", 5: \"Topic 6\", 6: \"Topic 7\", 7: \"Topic 8\", 8: \"Topic 9\", 9: \"Topic 10\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGDH2kFcLdPp"
   },
   "source": [
    "#### Visualisation des résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6083,
     "status": "ok",
     "timestamp": 1701424474296,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "qK4nN2RrLcN8",
    "outputId": "56e9deb2-6a3a-4213-d186-156af39de7a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 7799,
     "status": "error",
     "timestamp": 1701424593022,
     "user": {
      "displayName": "Andrea Leylavergne",
      "userId": "14980429397144187148"
     },
     "user_tz": -60
    },
    "id": "OKpoe4WSLmT3",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "1a36f3cc-ce2a-489f-8bae-200695642ab7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models import LdaMulticore\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2twjRHXfUb4W"
   },
   "source": [
    "## Identification du sujet le plus important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaYm7rN62_Dg"
   },
   "source": [
    "### Réduction de dimension avec TSNE et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABzEFV4VfDlg"
   },
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed8rnc4qwPY9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Extraire les distributions de sujets pour chaque document\n",
    "doc_topic_vectors = [lda_model[doc] for doc in bow_corpus]\n",
    "\n",
    "# Créer une liste de dictionnaires pour chaque document\n",
    "topic_data = []\n",
    "\n",
    "for topics in doc_topic_vectors:\n",
    "    cleaned_topics = {f\"Topic {t[0]}\": t[1] if not np.isnan(t[1]) else 0.0 for t in topics}\n",
    "    topic_data.append(cleaned_topics)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_topic_sents_keywords = pd.DataFrame(topic_data)\n",
    "\n",
    "# Vérifier et remplacer les NaN dans l'ensemble du DataFrame\n",
    "df_topic_sents_keywords = df_topic_sents_keywords.fillna(0.0)\n",
    "\n",
    "# Appliquer t-SNE pour réduire la dimension à 2\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_lda = tsne_model.fit_transform(df_topic_sents_keywords)\n",
    "\n",
    "# Créer un DataFrame pour les résultats t-SNE\n",
    "tsne_df = pd.DataFrame(tsne_lda, columns=['tsne1', 'tsne2'])\n",
    "\n",
    "# Visualiser les résultats avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='tsne1', y='tsne2', data=tsne_df)\n",
    "plt.title('t-SNE Visualization of LDA Topics for Documents')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Extraire les distributions de sujets pour chaque document\n",
    "doc_topic_vectors = [lda_model[doc] for doc in bow_corpus]\n",
    "\n",
    "# Créer une liste de dictionnaires pour chaque document\n",
    "topic_data = []\n",
    "\n",
    "for topics in doc_topic_vectors:\n",
    "    cleaned_topics = {f\"Topic {t[0]}\": t[1] if not np.isnan(t[1]) else 0.0 for t in topics}\n",
    "    topic_data.append(cleaned_topics)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_topic_sents_keywords = pd.DataFrame(topic_data)\n",
    "\n",
    "# Vérifier et remplacer les NaN dans l'ensemble du DataFrame\n",
    "df_topic_sents_keywords = df_topic_sents_keywords.fillna(0.0)\n",
    "\n",
    "# Appliquer t-SNE pour réduire la dimension à 2\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_lda = tsne_model.fit_transform(df_topic_sents_keywords)\n",
    "\n",
    "# Créer un DataFrame pour les résultats t-SNE\n",
    "tsne_df = pd.DataFrame(tsne_lda, columns=['tsne1', 'tsne2'])\n",
    "\n",
    "# Ajouter une colonne pour le nom du topic principal (Top 1)\n",
    "tsne_df['Top_Topic'] = df_topic_sents_keywords.idxmax(axis=1)\n",
    "\n",
    "# Ajouter une colonne pour la couleur du topic principal\n",
    "topic_colors = {\n",
    "    'Topic 0': 'red',\n",
    "    'Topic 1': 'green',\n",
    "    'Topic 2': 'blue',\n",
    "    'Topic 3': 'purple'\n",
    "}\n",
    "\n",
    "tsne_df['Color'] = tsne_df['Top_Topic'].map(topic_colors)\n",
    "\n",
    "# Visualiser les résultats avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='tsne1', y='tsne2', hue='Color', data=tsne_df, palette=topic_colors.values(), legend='full')\n",
    "\n",
    "# Ajouter des annotations pour les points\n",
    "#for idx, row in tsne_df.iterrows():\n",
    "    #plt.annotate(row['Top_Topic'], (row['tsne1'], row['tsne2']), fontsize=8)\n",
    "\n",
    "plt.title('t-SNE Visualization of LDA Topics for Documents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_ATZGpN9YXU"
   },
   "source": [
    "## Analyse de sentiments avec TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK2m-LnkBp9O"
   },
   "outputs": [],
   "source": [
    "# import de la librairie\n",
    "! pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-MqoAMjq-0n"
   },
   "outputs": [],
   "source": [
    "import textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Calcule de la polarité\n",
    "df_1['polarity'] = df_1['Processed_Text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df_1['subjectivity'] = df_1['Processed_Text'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "\n",
    "# Définir les étiquettes de sentiment en fonction de la polarité\n",
    "df_1['Sentiment'] = df_1['polarity'].apply(lambda x: 'Positif' if x > 0 else ('Négatif' if x < 0 else 'Neutre'))\n",
    "df_1.head(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUMWVZlptmzl7/DQy/VFwS",
   "machine_shape": "hm",
   "mount_file_id": "1LQGz_CsL-1ob6sBZfGUq0v6Y84EqHk4p",
   "provenance": [
    {
     "file_id": "1guOEfybCCYMf6V6EFP0i9Vj5PxGkp9mE",
     "timestamp": 1701580873259
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "kernel_6",
   "language": "python",
   "name": "kernel_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
